http://blog.csdn.net/zouxy09/article/details/8195017(生成模型和判别模型区别）

http://blog.csdn.net/u011067360/article/details/22879807(贝叶斯学习、MAP、ML - TH,原文打不开，所以贴过来）

在机器学习中，通常我们感兴趣的是在给定训练数据 D 时，确定假设空间 H 中的最佳假设。

所谓最佳假设，一种办法是把它定义为在给定数据 D 以及 H 中不同假设的先验概率的有关知识条件下的最可能（most probable）假设。

贝叶斯理论提供了计算这种可能性的一种直接的方法。更精确地讲，贝叶斯法则提供了一种计算假设概率的方法，它基于假设的先验概率、给定假设下观察到不同数据的概率、以及观察的数据本身。

要精确地定义贝叶斯理论，先引入一些记号。

1、P ( h )来代表还没有训练数据前，假设 h 拥有的初始概率。 P ( h )常被称为 h 的先验概率（prior probability ），它反映了我们所拥有的关于 h 是一正确假设的机会的背景知识。如果没有这一先验知识，那么可以简单地将每一候选假设赋予相同的先验概率 。

2、P ( D )代表将要观察的训练数据 D 的先验概率（换言之，在没有确定某一假设成立时， D 的概率）。

3、P ( D | h )代表假设 h 成立的情形下观察到数据 D 的概率。更一般地，我们使用 P ( x | y )代表给定 y 时 x 的概率。

在机器学习中，我们感兴趣的是 P ( h | D )，即给定训练数据 D 时 h 成立的概率。

P ( h | D )被称为 h 的后验概率（posteriorprobability），因为它反映了在看到训练数据 D 后 h 成立的置信度。

应注意，后验概率 P ( h | D )反映了训练数据 D 的影响；相反，先验概率 P ( h )是独立于 D 的。

贝叶斯法则是贝叶斯学习方法的基础，因为它提供了从先验概率 P ( h )以及 P ( D )和 P ( D | h )计算后验概率 P ( h | D )的方法。

贝叶斯公式

直观可看出， P ( h | D )随着 P ( h )和 P ( D | h )的增长而增长。同时也可看出 P ( h | D )随 P ( D )的增加而减少，这是很合理的，因为如果 D 独立于 h 被观察到的可能性越大，那么 D 对 h 的支持度越小。
极大后验（maximum a posteriori, MAP）假设：

学习器考虑候选假设集合 H 并在其中寻找给定数据 D 时可能性最大的假设 h ∈ H （或者存在多个这样的假设时选择其中之一）这样的具有最大可能性的假设被称为极大后验（maximum a posteriori, MAP）假设。确定MAP假设的方法是用贝叶斯公式计算每个候选假设的后验概率。

更精确地说当下式成立时，称 h MAP 为—MAP假设：


（在最后一步我们去掉了 P ( D )，因为它是不依赖于 h 的常量）

极大似然（maximum likelihood，ML）假设

在某些情况下，可假定 H 中每个假设有相同的先验概率（即对 H 中任意 h i 和 h j ， P ( h i )= P ( h j )）。这时可把上式进一步简化，只需考虑 P ( D | h )来寻找极大可能假设。 P ( D | h )常称为给定 h 时数据 D 的似然度（likelihood），而使P( D | h )最大的假设被称为极大似然（maximum likelihood，ML）假设 h ML 。


为了使上面的讨论与机器学习问题相联系，我们把数据 D 称作某目标函数的训练样例，而把 H 称为候选目标函数空间。

实际上，贝叶斯公式有着更为普遍的意义。它同样可以很好地用于任意互斥命题的集合 H ，只要这些命题的概率之和为1（例如：“天空是兰色的”和“天空不是兰色的”）。有时将 H 作为包含目标函数的假设空间，而 D 作为训练例集合。其他一些时候考虑将 H 看作一些互斥命题的集合，而 D 为某种数据。
